{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cfc6fef-e255-4872-ba32-4c09ee9cae60",
   "metadata": {},
   "source": [
    "# Exercise 3 - Features and clustering \n",
    "\n",
    "In this exercise we will try to separate some of the objects we collected using different clustering techniques. We will do this using features, and embeddings: Image embeddings are high-dimensional numerical representations of visual content that condense the essential features of an image into a vector of fixed size, which may include shapes, colors, patterns, and spatial hierarchies, etc.  \n",
    "\n",
    "<center><img src=../assets/butterfly-rois.png width=\"800\" ></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ca03e-f086-4ffa-bd5d-e4c33c982644",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "First we load some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6361dc-2660-400b-8d63-5279aeeca4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import phenopype as pp\n",
    "from torchvision import transforms\n",
    "from torch import nn \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(r\"C:\\Users\\mluerig\\Downloads\\phenomics-workshop-aussois-main\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b901f7f-7fb1-4075-9d3d-1df8b4168692",
   "metadata": {},
   "source": [
    "We will use a small model (Resnet50) that was trained on the ImageNet dataset. The model can be downloaded using timm (Pytorch image models library: https://timm.fast.ai/). After downloading the model we need to create a transform function to convert our image array to a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe9bdc-d648-4ce8-8e45-4bb7f1dbb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download model via timm\n",
    "model = timm.create_model(model_name=\"resnet50\", pretrained=True)\n",
    "\n",
    "## set into evaluation mode (changes its behavior from training to inference)\n",
    "model = model.eval()\n",
    "\n",
    "## get the penultimate (fully connected) layer used for classification\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1]) ## second to \n",
    "\n",
    "# Define the necessary transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts the image to a tensor and scales the values to [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978f482-9d76-4dd5-8594-1c18ef6b4d9c",
   "metadata": {},
   "source": [
    "The following piece of code will 1) load a saved ROI and apply the mask from the alpha-channel to the RGB channel, 2) tranform the image to a tensor, and 3) extract the image embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c712be-2ee3-41f7-bca9-8dd6a660bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) load image with alpha channel and apply mask \n",
    "img_path = r\"C:\\Users\\mluerig\\Downloads\\phenomics-workshop-aussois-main\\phenopype\\rois\\mcz-ent-170049_1.jpg_05.png_002.png\"\n",
    "img_RGBA = pp.load_image(img_path)    \n",
    "img, alpha_channel = img_RGBA[:,:,:3], img_RGBA[:,:,3]\n",
    "mask = alpha_channel > 0\n",
    "img[~mask]=0\n",
    "\n",
    "# 2) apply the transformations\n",
    "img_tens = transform(Image.fromarray(img))\n",
    "img_tens = img_tens.unsqueeze(0)\n",
    "\n",
    "# 3) extract_embeddings, normalize, convert back to array\n",
    "features = model(img_tens)\n",
    "features = nn.functional.normalize(features)\n",
    "features = features.detach().numpy()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18884a7f-de6d-43e0-9c53-ee0d99e3e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.show_image(img_RGBA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b66f6-30a1-4d80-b3de-85a5f1160ee2",
   "metadata": {},
   "source": [
    "Let's build a little pipeline to do this for all our ROIs - first we create a list of all images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaea2a9-b3b8-4cc9-be23-57f57e71ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_masks = os.listdir(\"phenopype/rois\")\n",
    "list_all_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4d197-b1a1-4f8f-a6e2-2db07808f2cb",
   "metadata": {},
   "source": [
    "Then we place our code for inference into a loop whose progress we trace with a progress bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6113e7a-c671-4ec0-8251-11f7edb63a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty dictionary to store embeddings\n",
    "results_dict_embeddings = dict()\n",
    "\n",
    "## create progress bar\n",
    "pbar = tqdm(total=len(list_all_masks), position=0, leave=False, desc=\"Embedding images\")\n",
    "\n",
    "## run \n",
    "for img_name in list_all_masks:\n",
    "    \n",
    "        ## construct img_path\n",
    "        img_path = os.path.join(\"phenopype\", \"rois\", img_name)\n",
    "\n",
    "        ## 1) load image\n",
    "        img_RGBA = pp.load_image(img_path)    \n",
    "        img, alpha_channel = img_RGBA[:,:,:3], img_RGBA[:,:,3]\n",
    "        mask = alpha_channel > 0\n",
    "        img[~mask]=0\n",
    "        \n",
    "        # 2) apply the transformations\n",
    "        img_tens = transform(Image.fromarray(img))\n",
    "        img_tens = img_tens.unsqueeze(0)\n",
    "        \n",
    "        # 3) extract_embeddings, normalize, convert back to array\n",
    "        features = model(img_tens)\n",
    "        features = nn.functional.normalize(features)\n",
    "        features = features.detach().numpy()\n",
    "        results_dict_embeddings[img_name] = features[0]\n",
    "    \n",
    "        ## update progress bar\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b39e21-19b5-4cce-8851-05a3e6cc786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save results\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "results_df = pd.DataFrame.from_dict(results_dict_embeddings, orient=\"index\")\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.rename(columns={'index': 'mask_name'}, inplace=True)\n",
    "results_df.to_csv(\"data/embeddings.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bac7bb-7b70-421d-beba-a406d3259e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d9aab-a5a5-4989-b4ac-7e2467051420",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We need to load some more packages for clustering and plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa41b5f-3d81-40b0-941a-d15e4bb3432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "from bokeh.io import show\n",
    "from bokeh.models import Div, HoverTool, CustomJS, ColumnDataSource, tickers\n",
    "from bokeh.plotting import figure, output_file\n",
    "from bokeh.transform import linear_cmap, factor_cmap\n",
    "from bokeh.palettes import all_palettes\n",
    "from bokeh import layouts\n",
    "\n",
    "def get_offset_img(path, zoom=1):\n",
    "    image = pp.load_image(path, mode=\"rgb\")\n",
    "    image_resized = pp.resize_image(image, max_dim=250)\n",
    "    return OffsetImage(image_resized, zoom=zoom)\n",
    "\n",
    "def assign_color(name):\n",
    "    if name.startswith(\"red\"):\n",
    "        return 'red'\n",
    "    elif name.startswith(\"blue\"):\n",
    "        return 'blue'\n",
    "    elif name.startswith(\"green\"):\n",
    "        return 'green'\n",
    "    else:\n",
    "        return 'purple'\n",
    "\n",
    "def sample_group(group, fraction, seed=42):\n",
    "    num_samples = max(1, int(len(group) * fraction))\n",
    "    return group.sample(n=num_samples, replace=False, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb364b-ed5c-4bd1-9a3a-21566648c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can cluster the embeddings...\n",
    "data_emb = pd.read_csv(\"data/embeddings.csv\")\n",
    "rename_dict = {col: f'emb_{col}' for col in data_emb.columns[1:]}\n",
    "data_emb = data_emb.rename(columns=rename_dict)\n",
    "cols_emb = [col for col in list(data_emb) if col.startswith(\"emb\")]\n",
    "data_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4dcb5c-6ce9-47f8-8a86-560057be5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ... for instance using tSNE (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "## this can take A LOT of time\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=100)  # n_components=2 for 2D visualization\n",
    "transformed_data = tsne.fit_transform(data_emb[cols_emb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3c42d-ee2c-4e9b-a353-da01e1706079",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prep plot df\n",
    "data_plot = pd.concat([data_emb[[\"mask_name\"]], pd.DataFrame(\n",
    "    data = transformed_data, columns = ['Dim1', 'Dim2']) ], axis=1)\n",
    "\n",
    "data_plot['mask_path'] = data_plot.apply(lambda row: os.path.join(\"phenopype\",\"rois\", row['mask_name']), axis=1)\n",
    "data_plot['mask_path_plots'] = data_plot.apply(lambda row: os.path.join(\"..\", row['mask_path']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9abd1-dcce-4bd1-9203-aed0e33234ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's plot the embeddings!\n",
    "fig, ax = plt.subplots(figsize=(7, 7))  \n",
    "plt.tight_layout(pad=3)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(\"TSNE - embeddings\")\n",
    "ax.set_xlabel(\"Dimension 1\")\n",
    "ax.set_ylabel(\"Dimension 2\")\n",
    "scatter = ax.scatter(data_plot['Dim1'], data_plot['Dim2'], s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c12c2-592e-4902-b644-95efdda7a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "## with pictograms\n",
    "fig, ax = plt.subplots(figsize=(7, 7))  \n",
    "plt.tight_layout(pad=3)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(\"TSNE - embeddings\")\n",
    "ax.set_xlabel(\"Dimension 1\")\n",
    "ax.set_ylabel(\"Dimension 2\")\n",
    "scatter = ax.scatter(data_plot['Dim1'], data_plot['Dim2'], s=20)\n",
    "\n",
    "pbar = tqdm(total=len(data_plot), position=0, leave=False, desc=\"Plotting...\")\n",
    "for idx, row in data_plot.iterrows():\n",
    "    ab = AnnotationBbox(get_offset_img(row[\"mask_path\"], zoom=0.1), \n",
    "        (row[\"Dim1\"], row[\"Dim2\"]), frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "    pbar.update(1)\n",
    "\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "fig.savefig(\"plots/embeddings_pictograms.png\", dpi=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5eadf-5066-4d8d-a9b1-3648540bf1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare htlm output \n",
    "filepath = \"plots/embeddings_interactive.html\"\n",
    "output_file(filepath)\n",
    "\n",
    "## convert to datasource\n",
    "ds_points = ColumnDataSource(data=dict(data_plot))\n",
    "\n",
    "## add hover tool\n",
    "hover=HoverTool(\n",
    "        tooltips = [\n",
    "        (\"mask_name\", \"@mask_name\"),\n",
    "        ]\n",
    ")\n",
    "\n",
    "## add interactive panel\n",
    "div = Div(text=\"\")\n",
    "hover.callback = CustomJS(args=dict(div=div, ds=ds_points), code=\"\"\"\n",
    "    const hit_test_result = cb_data.index;\n",
    "    const indices = hit_test_result.indices;\n",
    "    if (indices.length > 0) {\n",
    "        div.text = `<img \n",
    "        src=\"${ds.data['mask_path_plots'][indices[0]]}\"\n",
    "        style=\"float: left; margin: 0px 15px 15px 0px; max-width: 500px; max-height: 500px; width: auto; height: auto;\"\n",
    "        border=\"2\"\n",
    "        />`;\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "## create scatter panel\n",
    "p = figure(tools=[\n",
    "    \"pan\", 'reset', 'wheel_zoom', 'box_zoom', \"lasso_select\", \"tap\", hover], \n",
    "    active_scroll=\"wheel_zoom\",active_drag =\"pan\", output_backend=\"webgl\",\n",
    "    width=1200, height=800, match_aspect=True,\n",
    "    x_axis_label=\"Dim1\", y_axis_label=\"Dim2\") \n",
    "p.xaxis.ticker = tickers.SingleIntervalTicker(interval=1)\n",
    "p.yaxis.ticker = tickers.SingleIntervalTicker(interval=1)\n",
    "p.scatter(x='Dim1', y='Dim2', source=ds_points, size=15)\n",
    "\n",
    "## plot\n",
    "layout = layouts.row(p, div)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485f692-b059-49ae-a1cb-f17115f375ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"plots\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8543c3f-1ab5-4c50-bba9-e69daeb62c16",
   "metadata": {},
   "source": [
    "## A more complete example\n",
    "\n",
    "Let's load segmentation masks, handcrafted features and embeddings I have created on a larger Junonia dataset using a pretrained encoder (ViT). First we load all files, then merge them together:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa713b1-a0c3-4941-a7e7-f5cd501603c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.read_csv(\"data_raw/data_features.csv\")\n",
    "df_emb = pd.read_csv(\"data_raw/data_embeddings.csv\")\n",
    "\n",
    "data_all = pd.merge(df_feat, df_emb, on=['mask_name',\"species\"], how='outer')\n",
    "\n",
    "cols_emb = [col for col in list(data_all) if col.startswith(\"emb\")]\n",
    "cols_feat = [col for col in list(data_all) if col.startswith((\"shape\",\"red\",\"green\",\"blue\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e5307-9a38-4899-ada7-5fcbc74d5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prep pca\n",
    "scaler = StandardScaler()\n",
    "X_scaled_main = scaler.fit_transform(data_all[cols_emb])\n",
    "X_scaled_support = scaler.fit_transform(data_all[cols_feat].values)\n",
    "\n",
    "## do pca\n",
    "pca = PCA(n_components=len(cols_emb))\n",
    "principalComponents = pca.fit_transform(X_scaled_main)\n",
    "components_main = pca.transform(X_scaled_main) \n",
    "\n",
    "##Calculate the loadings (correlations) of the supplementary variables on the principal components\n",
    "supp_loadings = np.dot(components_main.T, X_scaled_support) / (X_scaled_support.shape[0] - 1)\n",
    "data_supp = pd.DataFrame(supp_loadings.T[:,:2], columns=[\"Dim1\",\"Dim2\"])\n",
    "data_supp[\"features\"] = cols_feat\n",
    "data_supp['Color'] = data_supp['features'].apply(assign_color)\n",
    "\n",
    "## percent variance explained\n",
    "var_explained = pca.explained_variance_ratio_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4af19-c2a5-4717-9bc0-23caeffb11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## do tsne (takes a few seconds)\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=100)  # n_components=2 for 2D visualization\n",
    "transformed_data = tsne.fit_transform(data_all[cols_emb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c5410-f08f-4ae0-b444-fc141a7ccc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prep plot df\n",
    "data_plot = pd.concat([data_all[[\"mask_name\", \"species\"]], pd.DataFrame(data = transformed_data, columns = ['Dim1', 'Dim2'])], axis=1)\n",
    "data_plot = pd.concat([data_plot, pd.DataFrame(data = components_main[:,:2], columns = ['PC1', 'PC2'])], axis=1)\n",
    "data_plot['mask_path'] = data_plot.apply(lambda row: os.path.join(\"data_raw\",\"segmentation_masks\", row['species'], row[\"mask_name\"]), axis=1)\n",
    "data_plot['mask_path_plots'] = data_plot.apply(lambda row: os.path.join(\"..\",\"data_raw\",\"segmentation_masks\", row['species'], row[\"mask_name\"]), axis=1)\n",
    "\n",
    "##\n",
    "mask_subset = data_plot.groupby('species').apply(sample_group, fraction=0.10, seed=42)\n",
    "mask_subset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# colors for bokeh\n",
    "unique_species = data_plot['species'].unique()\n",
    "color_map = plt.get_cmap(\"nipy_spectral\", len(unique_species))\n",
    "color_palette = [mcolors.to_hex(color_map(i)) for i in range(len(unique_species))]\n",
    "color_mapper = factor_cmap('species', palette=color_palette, factors=unique_species)\n",
    "\n",
    "## colors for matplotlib\n",
    "data_supp['Color'] = data_supp['features'].apply(assign_color)\n",
    "category_to_color = {category: color for category, color in zip(unique_species, color_palette)}\n",
    "data_plot['Color_species'] = colors = data_plot['species'].map(category_to_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3838b69-6967-4ad1-bac1-855724bdc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare htlm output \n",
    "filepath = \"plots/embeddings_interactive_complete-ex.html\"\n",
    "output_file(filepath)\n",
    "\n",
    "## convert to datasource\n",
    "ds_points = ColumnDataSource(data=dict(data_plot))\n",
    "\n",
    "## add hover tool\n",
    "hover=HoverTool(\n",
    "        tooltips = [\n",
    "        (\"species\", \"@species\"),\n",
    "        ]\n",
    ")\n",
    "\n",
    "## add interactive panel\n",
    "div = Div(text=\"\")\n",
    "hover.callback = CustomJS(args=dict(div=div, ds=ds_points), code=\"\"\"\n",
    "    const hit_test_result = cb_data.index;\n",
    "    const indices = hit_test_result.indices;\n",
    "    if (indices.length > 0) {\n",
    "        div.text = `<img \n",
    "        src=\"${ds.data['mask_path_plots'][indices[0]]}\"\n",
    "        style=\"float: left; margin: 0px 15px 15px 0px; max-width: 500px; max-height: 500px; width: auto; height: auto;\"\n",
    "        border=\"2\"\n",
    "        />`;\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "## create scatter panel\n",
    "p = figure(tools=[\n",
    "    \"pan\", 'reset', 'wheel_zoom', 'box_zoom', \"lasso_select\", \"tap\", hover], \n",
    "    active_scroll=\"wheel_zoom\",active_drag =\"pan\", output_backend=\"webgl\",\n",
    "    width=1200, height=800, match_aspect=True,\n",
    "    x_axis_label=\"Dim1\", y_axis_label=\"Dim2\") \n",
    "p.xaxis.ticker = tickers.SingleIntervalTicker(interval=50)\n",
    "p.yaxis.ticker = tickers.SingleIntervalTicker(interval=50)\n",
    "p.scatter(x='Dim1', y='Dim2', source=ds_points, color=color_mapper, size=10)\n",
    "\n",
    "## plot\n",
    "layout = layouts.row(p, div)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecd412-d33e-4635-b74c-f519d918b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## scatter plot\n",
    "fig, ax = plt.subplots(figsize=(15, 15))   \n",
    "plt.tight_layout(pad=3)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title(\"PCA - Junonia all\")\n",
    "ax.set_xlabel(f\"Dimension 1 - ({var_explained[0]*100:.2f}% variance explained)\")\n",
    "ax.set_ylabel(f\"Dimension 2 - ({var_explained[1]*100:.2f}% variance explained)\")\n",
    "scatter = ax.scatter(data_plot['PC1'], data_plot['PC2'], c=data_plot[\"Color_species\"], s=15)\n",
    "\n",
    "## pictograms\n",
    "pbar = tqdm(total=len(mask_subset), position=0, leave=False, desc=\"Plotting...\")\n",
    "for idx, row in mask_subset.iterrows():\n",
    "    ab = AnnotationBbox(get_offset_img(row[\"mask_path\"], zoom=0.2), \n",
    "        (row[\"PC1\"], row[\"PC2\"]), frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "    pbar.update(1)\n",
    "\n",
    "## arrows\n",
    "data_supp_sub = data_supp[data_supp[\"features\"].isin([\n",
    "    \"shape_area\", \"shape_diameter\", \"shape_circularity\",\"shape_hu3\", \n",
    "    \"red_mean\", \"red_variance\", \"red_uniformity\",\n",
    "    \"green_mean\", \"green_variance\", \"green_uniformity\",\n",
    "    \"blue_mean\", \"blue_variance\", \"blue_uniformity\"])]\n",
    "amod = 5\n",
    "for idx, (load_x, load_y, feature, Color) in data_supp_sub.iterrows():\n",
    "    ax.arrow(0, 0, load_x*amod, load_y*amod, color=Color, alpha=1, width=0.1, head_width=0.5, length_includes_head=True, zorder=4)\n",
    "    ax.text(load_x*amod, load_y*amod, feature, color=Color,fontsize=8, zorder=5,\n",
    "            bbox=dict(facecolor=\"white\", alpha=1, edgecolor=Color, linewidth=0.5, boxstyle=\"Round\"),\n",
    "            ha='right' if load_x < 0 else 'left', va='bottom' if load_y < 0 else 'top')\n",
    "\n",
    "fig.savefig(\"plots/embeddings_pictograms_supp.png\", dpi=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa52a51-d374-4a90-ae72-23f473c785d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
